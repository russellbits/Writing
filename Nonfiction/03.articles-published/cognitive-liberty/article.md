---
Title: Notes on Cognitive Liberty
Date: 29-12-2023
---

# Notes on Cognitive Liberty

My academic background includes study of the subjects of neurology and artificial intelligence. However, I have kept those fields in two different compartments in my head until now. The reason for the shift in my thinking is that these two subjects are now inextricably intertwined. I came to this conclusion thanks to a podcast called “[Ologies](https://www.alieward.com/ologies).” In particular, [episode 336](https://www.alieward.com/ologies/neurotechnology) titled “[Neurotechnology (AI + BRAIN TECH) with Dr. Nita Farahany](https://www.alieward.com/ologies/neurotechnology).” 

My interest lay with Dr. Nita Farahany, because given the title of the episode, you might presume her to be an artificial intelligence researcher or a neuroscientist. But regard! She is a *lawyer*, and because I have been on the side of those who say that A.I. is potentially one of our most dangerous technologies to date, and that it could spell the end of humankind if mishandled, I wanted to know more.

It doesn’t have to be a terminator-style extinction. An A.I. tasked with making paper clips and given no upper limit might learn how to divert resources to its task to the point of disrupting our economy or even starving us to death. So, I’m for legislation and regulation, but that’s where my pontificating stopped. I have no suggestions for how to curb these new A.I. programs that are emerging. What regulations will be good enough?

We’ve seen [how effective](https://en.wikipedia.org/wiki/West_Fertilizer_Company_explosion) OSHA, the EPA, and ATF were when a fertilizer factory exploded after years of ignoring regulations and falsifying reports, killing 15 people, injuring 160 people, and destroying 150 nearby buildings. With A.I. one such mistake could cause far more damage. Plus, it’s a lot easier for a teenager to build an open source AI than it is for them to [build a nuclear reactor](https://en.wikipedia.org/wiki/David_Hahn)—not that they can’t do it, but building an A.I. doesn’t require you to get your hands on regulated, hazardous materials.

Dr. Farahany has some ideas about where to begin and she really got me thinking. For instance, she used the term “cognitive liberty”—a term that is Orwellian in its nature simply because it begs questions like can our minds be read for abuse in the justice system? Do we have a right to the privacy of our own thoughts. Twenty, or even ten years ago, why would this be a concern? Now, however, when A.I. is helping the medical world [see what you see inside your brain](https://www.science.org/content/article/ai-re-creates-what-people-see-reading-their-brain-scans) and interpret what is happening, we are coming much closer to an era where even your dreams could be rendered. So, how many steps does it take for images to be injected into the brain in such a fashion? How far are we from, say, companies advertising in our dreams? Ugh. We definitely need some boundaries here.

Some of these potential laws seem obvious after the fact. Identification is one. If you are communicating with a generative A.I., it must identify itself as one. There are currently crimes in which [A.I. voices resembling a loved one](https://arstechnica.com/tech-policy/2023/03/rising-scams-use-ai-to-mimic-voices-of-loved-ones-in-financial-distress/) are scamming individuals for money—to the tune of $11 million in 2022. Are criminals going to obey laws that make them identify their A.I. partners in crime? No. But the existence of such a law could go a long way to making it easier for victims to re-coup their losses.

In China, the operators of bullet trains are [made to wear EEG helmets](https://www.news.com.au/lifestyle/real-life/china-introduces-artificial-intelligence-thought-police-to-improve-worker-efficiency-military-loyalty/news-story/cbd106386c26843229e49ccedb4be218) that allow others to monitor their attention-levels. As much as China gets a well-deserved rap for being authoritarian, I can think of a few [environmental](https://en.wikipedia.org/wiki/Exxon_Valdez_oil_spill) [disasters](https://www.trains.com/trn/news-reviews/news-wire/ntsb-engineer-in-2019-csx-collision-in-ohio-was-intoxicated-updated/) that might have been avoided with the use of such technology. But how far does this go? Should factory workers have to wear such devices to monitor productivity? They’re already pretty heavily monitored when it comes to bathroom use, so I think we can expect a company like Amazon to at least try this methodology.

Other companies will extol the virtues of these devices, but make no provisions for your privacy, because as with social media, the data you produce has sales value. I predict that what will happen is that the companies will offer the device to you as a method of, say, *improving your meditation regime*. They’ll also offer you the ability to track that information in the cloud and back it up safely for yourself. Buried somewhere in their terms of service will be a statement that says they have the right to do what they want with that data, including selling it. You may ask, “What harm is there in someone having data about my meditation habits and abilities?”

If you are indeed saying this, I must reply, “Have we learned nothing?” Of all the data hacking, stolen passwords, and identity thefts that occurred because of Facebook, one thing Facebook did that was trivial on its face and yet caused a lot of people crises, was simply to post your birthday on your profile page. Your birthday, your name and a list of people that you know—an identity thief’s paradise. Corporations are just not incentivized to take the necessary precautions to protect your data; they never will be. The bottom line will always be more important than the next best level of security.

Let’s say someone invents (and they will) a device that can tune into your Broca’s area. The Broca’s area is a small module of your brain that is responsible for generating speech. It should be possible to create a device that scans that area and lets you dictate text via thinking. Should that data be available to corporations to back up on the cloud, and aggregate and sell? Even if a corporation that sold such a device had high ethical standards and went to great lengths to *not* aggregate and sell your thoughts, could they really guarantee protection against determined hackers? Could they avoid being sold to another company that had no interest in protecting your thoughts? What sort of recourse would you have if you signed a End User License Agreement (EULA) and that companies’ lawyers protected the company (as they often do) by inserting a statement that says you don’t have the right to sue them if they lose or damage the data?

We need serious law in this area; not congress passing the buck to some new regulatory agency. This level of danger requires constitutional change. With the advent of A.I., we will not only see the technology present its own danger, but with its help, we will eventually see it enter our brains with ever-increasing fidelity. You have the right to free speech. Shouldn’t you have the right to free and private thought? Shouldn’t that right not be defended by a bureaucracy, but rather writ large as an amendment to the US Constitution? Perhaps, we can count on our A.I. overlords to change the constitution for us, since our Congress is so unlikely to do what’s necessary here.