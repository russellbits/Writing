# Weaponizing Bits: Twenty Years Later, the Drones Are Driving Themselves

In 1995, Nicholas Negroponte walked through customs with a laptop he valued at somewhere between one and two million dollars. The customs agents, examining the physical object—the plastic shell, the circuit boards, the atoms—estimated its worth at about $2,000. What they missed, of course, were the bits: the papers, the software, the unpublished research packed into that hard drive by a well-known MIT professor. The atoms were cheap. The bits were priceless.

I wrote about this anecdote twenty years ago in a piece about drones and 3D-printed guns, back when the weaponization of bits was still theoretical enough to feel like a thought experiment. The argument then was straightforward: if commerce was making the sneaking transition from atoms to bits—music, movies, books all dematerializing into data—then warfare was undergoing something similar but more unsettling. The bits of war were starting to have distinct effects on the material world, primarily through de-localization (drones making presence irrelevant) and disintermediation (3D printers making knowledge about manufacturing irrelevant).

Here's the thing I got wrong: I thought we'd have more time.

## From Pilot to Algorithm

When I wrote about drones in 2005, they were still mostly piloted remotely by humans. A soldier in Nevada could fly a Predator over Afghanistan, watch the feed, make decisions. The medium was hot—all that visual data transmitted back—but the cognitive load remained human. Someone still had to decide when to pull the trigger.

In Ukraine today, [drones track and engage enemies without human interaction](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/). True killer robots, operating autonomously. The Ukrainian military has demonstrated an [autonomous machine gun that uses AI to spot and target enemies](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/) moving in the field. It can identify targets on its own, though—for now—it still requires a human operator to authorize the kill shot.

For now.

The progression here follows exactly the pattern I sketched out two decades ago, but faster and more thoroughly than I imagined. Remember McLuhan's observation that all media are extensions of our corporeal bodies? Guns extend our arms (not incidentally why we call them arms). Drones extended our eyes and our reach. But AI-powered autonomous weapons are extending something more fundamental: our decision-making, our judgment about who lives and dies. We're not just removing soldiers from the battlefield anymore. We're removing human cognition from the kill chain entirely.

A U.S. general described AI's impact in Ukraine as "the most significant fundamental change in the character of war ever recorded in history." That's not hyperbole. That's just counting.

## The Fight-or-Flight Gap Widens

Twenty years ago, I wrote about how drone warfare was a cool medium compared to actual combat—it left out all the sensory details that would normally trigger our fight-or-flight response. No smell of cordite, no incoming fire, no chemical cascade of adrenaline and cortisol that fundamentally alters human decision-making. Fighting via drone was warfare with the visceral danger edited out, which made it cognitively easier to prosecute wars.

But at least humans were still making the decisions. At least there was still *someone* in the loop whose brain chemistry could theoretically be affected by what they were doing, even through a screen.

Now we're building systems where there's no one in the loop at all. The British company Malloy Aeronautics has developed [robot dogs that move stealthily through combat zones](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/), detecting enemy placements and minefields using thermal vision. China has demonstrated similar technology equipped with a machine gun and capable of engaging in combat. These aren't remote-controlled toys. They're autonomous systems making tactical decisions in real-time.

Here's what I got right in 2005: when soldiers aren't materially in harm's way, citizens lose the primary motivation to protest wars. We saw the precipitous drop in protests between Vietnam (with its draft) and Iraq (with its volunteer army). But I underestimated how far this could go. We're not just removing *our* children from danger anymore. We're removing human decision-making from the equation entirely, which means we're also removing human hesitation, human doubt, human mercy, and—perhaps most importantly—human accountability.

When an autonomous weapons system kills the wrong person, who goes to trial?

## The Knowledge Economy Crashes Into the Battlefield

The 3D printing angle I explored twenty years ago was about how bits make knowledge irrelevant. Before 3D printers, making a gun required specialized knowledge—metallurgy, mechanics, ballistics. After 3D printers, all you needed was a download and a button. Knowledge became transient, possessed by the network rather than by individuals.

I argued then that Congress's attempts to ban 3D-printed gun plans were doomed because banning knowledge has never worked in human history. Twenty years later, I stand by that argument, but I missed how the pattern would scale. It's not just about guns anymore.

The Ukrainian government set up [a funding platform called Brave1](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/) where companies can pitch defense technology products to investors. It's received thousands of submissions. When war broke out in a country with 300,000 people employed in the tech sector, startups pivoted overnight to military applications. The bits—the code, the algorithms, the trained models—flowed freely. Knowledge about building autonomous weapons systems became democratized in real-time.

This is the weaponization of bits at scale: not just plans for printing a single gun, but entire ecosystems of military AI, openly developed, rapidly iterated, and impossible to contain. The knowledge isn't in textbooks anymore. It's in GitHub repositories, in open-source machine learning frameworks, in the collective intelligence of the network itself.

Companies like [Palantir](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/) have reportedly provided data analytics tools for AI-augmented surveillance efforts in Ukraine. Clearview has supplied facial recognition technology to identify Russian soldiers crossing borders. We're building an infrastructure where the bits flow seamlessly from Silicon Valley boardrooms to active war zones, with barely a pause for ethical consideration in between.

## The Bullet Problem, Revisited

In 2005, I made what seemed like a clever argument: since 3D-printed plastic guns were basically worthless without ammunition, we should regulate the bullets, not the plans. The bits have no value without the atoms, right? The laptop is worthless without the hard drive's contents, but the contents can't kill anyone.

Except now the bits *can* kill. An autonomous drone doesn't care whether its explosive payload was manufactured in a factory or printed in someone's garage. The AI that guides it to target doesn't distinguish between official military ordnance and improvised devices. The bits—the targeting algorithms, the computer vision models, the decision-making neural networks—are now the primary weapon. The atoms are just delivery mechanisms.

This flips Negroponte's customs story on its head. The agents who examined his laptop and saw only $2,000 worth of plastic and circuits were wrong, yes, but they weren't dangerously wrong. Nobody died because they undervalued those bits. But when we undervalue the bits that guide autonomous weapons—when we treat them as mere software, as just another GitHub repository—we're making a catastrophically different kind of mistake.

The value is still in the bits. But now the bits have a body count.

## The Post-Cognitive Battlefield

McLuhan taught us that media are extensions and also amputations—when we extend one faculty, we diminish another. The written word extended memory but atrophied our oral traditions. The photograph extended vision but reduced our powers of observation. Television extended our reach but amputated our attention spans.

What does autonomous warfare extend, and what does it amputate?

It extends our capacity for violence beyond any previous boundary of space, time, or human endurance. A machine doesn't get tired, doesn't feel guilt, doesn't hesitate, doesn't question orders. It processes sensor data and executes decisions at speeds that make human reaction times look geological.

But what it amputates is something we might not realize is gone until it's too late: the friction of human judgment. The built-in latency of moral consideration. The evolutionary baggage of empathy that slows down our kill decisions just enough to allow for doubt.

We're not just building post-literate societies where reading rates plummet and attention spans shrink to goldfish durations. We're building post-cognitive battlefields where human thinking becomes optional, then obsolete, then—eventually—too slow to matter.

Steve Jobs called the computer a bicycle for the mind, a tool to amplify human intelligence. What's the metaphor for AI weapons? A motorcycle for murder? A rocket sled for killing that moves so fast the human mind can't keep up, can't course-correct, can't pull the emergency brake?

## The Equation Still Doesn't Balance

Twenty years ago, I ended with this: "The ability to take life, and take as much as possible, is getting cheaper. We need to balance the other side of the equation: we need to make life, and taking it, more expensive."

I had no solution then, and I have no better one now. But the equation has gotten worse.

Life is still cheap. Taking it is cheaper than ever. And now we're automating the process, removing even the minimal cost of human attention from the transaction. An autonomous weapon can kill while its programmers are asleep, while its operators are eating lunch, while the entire chain of command is blissfully unaware that a decision point even occurred.

[Some argue](https://www.forbes.com/sites/bernardmarr/2024/03/15/how-ai-is-used-in-war-today/) that robotic warfare could save human lives by keeping soldiers out of combat zones. But as the Forbes piece notes, what happens when one side runs out of robots? Do they surrender, or do they start sending humans back into the fray—humans who are now woefully unprepared for the speed and precision of autonomous systems?

There's also the risk of what military analysts call "tech-washing"—portraying modern warfare as clean, surgical, algorithmic. Robot dogs and autonomous drones make for compelling footage, but the vast majority of killing in Ukraine is still done by humans up to their knees in mud with artillery ringing in their ears. The bits make headlines. The atoms still do most of the dying.

## Does No One Read Science Fiction?

Here's the thing that bothers me most, twenty years later: we knew this was coming. Every science fiction writer worth their MacBook has been warning us about autonomous weapons for decades. Asimov gave us the Three Laws of Robotics in 1942. Philip K. Dick explored the nightmare of automated warfare in the 1950s. We've had eighty years of fictional cautionary tales, and we built the killer robots anyway.

History tells us that rules and principles of fair play prove inconsequential in warfare. McLuhan would say that's because the medium carries its own message regardless of our intentions. The message of autonomous weapons isn't about making war cleaner or safer or more ethical. The message is that we're willing to offload life-and-death decisions to systems we don't fully understand, can't completely control, and definitely can't recall once deployed.

In 2005, I worried about drones staying airborne and wondered when ground-based autonomous weapons would emerge. Well, they're here. The robot dogs are already deployed. The autonomous machine guns are already demonstrated. The kill chain is already shortened to the point where human judgment is optional.

And we're still having the same tired arguments about whether this is ethical, whether machines should be allowed to make kill decisions, whether tech companies should be developing tools that violate principles of "harmless" AI. Meanwhile, the bits keep flowing, the systems keep improving, and the battlefield keeps evolving faster than our ethical frameworks can adapt.

Twenty years ago, I ended by saying we needed to act as a society to emphasize the value of human life. I still believe that. But I'm significantly less optimistic about our ability to do it, because we're now racing against systems that operate at machine speed, make decisions in microseconds, and don't care about the value of anything at all.

The weaponization of bits isn't coming. It's here. It's deployed. It's autonomous.

And it's only going to get faster.