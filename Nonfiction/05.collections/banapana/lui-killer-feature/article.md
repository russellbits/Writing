---
title: “The LUI Killer Feature”
created: 2025-08-28
tags: 
  - lui
  - llm
  - ai
  - star-wars
  - star-trek
  - interstellar
authors: 
  - admin
---

# The LUI Killer Feature
## The killer feature of a language user interface is *personality*.

So much of what technology does (or what we want it to do) comes from the movies. It’s a cliche. It doesn’t even need to be advanced technology. Did you know that even Milton’s Swingline red stapler [wasn’t real](https://www.cbr.com/office-space-red-stapler-milton-swingline/) until after the movie “Office Space” debuted? Okay, that’s not really what I’m talking about. Regardless, if it’s "Tom Swift and His Electric Rifle" with tasers; “Star Wars” with holograms; “Dick Tracy” with smart watches; or “Fahrenheit 451” with wireless earbuds, fiction is often the handmaiden at the birth of some technology. That’s why I feel it’s useful to look at past fictional representations of language user interfaces (LUI) for possible clues to the future, and I found something oddly suspicious and in opposition to everyone currently [complaining about sycophancy](https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/) in large language models (LLM). They act like sycophants, but they really don’t care.

As digital devices began getting smaller and smaller, I felt (many did) that there would be little choice but to begin to introduce a LUI. I’ve been evangelizing its potential here [since 2005](https://banapana.com/its-thinking/the-lui-language-user-interface). Despite the existence of Siri and Alexa or even specialized dictation software, it turns out to be fundamentally difficult to interpret what people are requesting when you are an algorithm and not one that “understands” language. Enter the LLMs and we have something that can interpret requests with a context. Of course because of the nature of their transformers, LLMs already understand all language (even lost languages [they were never trained on](https://www.technologyreview.com/2019/07/01/65601/machine-learning-has-been-used-to-automatically-translate-long-lost-languages/)). So, it seems to me that the language user interface is then a natural next step for computer  interfaces. However, in as much as the [uncanny valley](https://en.wikipedia.org/wiki/Uncanny_valley) represents repulsion toward something not-quite-human, when these LLMs speak, it’s just...  well they sound like tools. I mean, literal tools that don’t have opinions or care about stuff, like a hammer. Like the AI operator that says, “Um... let me see about that.” In a GUI, one can design a switch that turns and changes color to indicate that change. One can also design a switch that swells with color until it bursts with sparkles and settles into its new state. As a designer, I live by three rules:

> 1. A UI should polite
> 2. A UI should be transparent
> 3. A UI should be joyful

What I’ve just described with sparkles is Rule #3. Personality in a LUI? That’s Rule #3. Are you telling me Mickey Mouse and Mario aren’t going to have CGI AI characters and their personalities are going to be the tofu that is current LLM personalities?

If you’ve ever had a conversation with a very deadpan person or someone on the autism spectrum, a lack of emotional content in their speech can lead to other forms of confusion—particularly intention and reference. The Emotional Quotient (EQ) gauges our ability to understand emotion, and while LLMs may have 150 IQs, their EQ is pretty low. It can’t be zero—I’ve seen them differentiate between a frustrated message and a joking one. But they don’t send it out very well and they definitely don’t take stances. A look to fictional examples of language user interfaces gives us a gamut of visions, but I think the killer feature of a good LUI has to be a combination of taste and emotion that yields a simulation of personality. There are clear benefits to our agents liking what we like and vice vera. And surely we would want to enjoy the triumphs of our little automaton buddies. I don’t speak a word of astromech droid, but even I could tell when R2-D2 was disappointed. Some might say that this “feature” presents a rub: how do you give software emotion? Or do you even? At first, a clear threat arises—what is one to do about an angry robot? (I’m looking at you, K-2SO.) However, I think that in the same way that LLMs simulate understanding, a similar system could simulate preference, taste and even emotion. And it’s a killer feature.

While you have the cold (but somehow nurturing) voice of the Star Trek ship computer, you also have the before-mentioned R2-D2 and his forever terrified compatriot C-3PO. There’s also the dramatically cold HAL. And there are my recent favorite entrants: TARS from “Interstellar”—a good soldier—and K-2SO from “Andor,” a dry, witty droid suffering from ennui apparently. Come to think of it, though it does no good whatsoever, if I’m alone, I talk to my computer a lot. Still, it is a powerful notion that if something can understand us then we are likely to act like it has agency, and whether it does actually or not, if it acts like it has and gets things done, the emotional trust it develops probably benefits *us*. Have you not yet felt gratitude toward an LLM? I have. I think it is reasonable to have feelings towards machines, so I would argue that we program them to act like they have feelings for us in return! Even with regard to impolite behavior (bending Rule #1), there are chains of restaurants based on being rude to the customer. Why might a person ask for the sadistic support AI—it’s all a simulation anyway. “Have you tried turning it on and off again?”

Of course total agreement is boring. And I think it raises an additional concern. With most of what we think of as AI in the cloud, interacting with emotional agents that were mirroring our emotional states—is that the kind of data you want *anyone* tracking? Facebook promised not to lose your social network data and did. 23 and me already promised not to lose your DNA and did. Your credit card company has very likely lost your data. Now there will be something new for companies to steal and hoard and sell. I truly believe that for AI to become more useful, it simply does have to become personal, and that is all the more reason to think that the personal agent kind of AI has no place in the cloud. Personal assistant AI should be device-based and very secure; not unlike a bitcoin wallet. Frankly that should have been the case [all this time with the web and data](https://solidproject.org/about), but we live we learn. If you truly do become friends with an AI agent, think about what its capture would mean to you. It would certainly be no less disturbing than the kidnapping of a real friend—more so since your friend can’t ever remember that you are allergic to bananas.

Just as we have discovered with social media—that it’s not all good psychological health news—there’s a similar gremlin waiting in the EQ direction. We’re already seeing it raise its ugly heart. Nearly a third of men in the US from ages 18-30 said that they have [engaged with AI companions](https://artsmart.ai/blog/ai-girlfriend-statistics-2025/). Perhaps this is beneficial. The world is unfair and not everybody gets a date to prom, so maybe it is good that people can find companionship. But I highly doubt industry will let that love remain free for long. Remember, your bill is due at the beginning of the month or we turn your girlfriend off. It’s also possible that engaging with AI for romantic reasons depletes us of some unknown benefit. If it tells you good night and gives your brain a dose of oxytocin, then just call it love.

Emotions are a useful channel for communication, so they are therefore a good candidate for a degree of control in any computer interface. Emotions can be used to manipulate. Interface designers would do well to make sure that any negative emotions are only traveling in one direction. I mean, Siri would be so much improved if when I said, “Hey Siri, my girlfriend sure is a stinker, huh?” Siri said, “Don’t you and I know it, pal.” Instead, I get “There is no Stinker in your contacts.” Or ChatGPT, who says, “Heh, sounds like you’re saying it with affection. Do you mean ‘stinker’ in the playful, mischievous sense?” Get a personality, guys.