# Energy Economics and the Big Brain

I have a problem with the accounting of most AI companies right now.

What does it cost a company to employ a person versus subscribe to an AI. When will the price of human labor be outweighed by AI labor?

I believe it was Karen Hao that pointed out something interesting about resource usage for LLMs as opposed to human brains

Comparison Table
Human Brain vs. Popular AI LLMs

| Header 1 | Header 2 | Header 3 |
|----------|----------|----------|
| Row 1 Col 1 | Row 1 Col 2 | Row 1 Col 3 |
| Row 2 Col 1 | Row 2 Col 2 | Row 2 Col 3 |


|  | Human |	ChatGPT |	Claude |	Grok |	Llama | Biotech |
|--            |--      |--      |--     |--      |-- |
| Substrate | Cells | GPUs | GPUs | GPUs | GPUs | Cells |
| Teraflops |	~100,000 | ~560 |	N/S |	N/S |	N/S | N/S |
| Electricity Usage | ≈20 W × 1 hr = 0.02 kWh (full body ~100 W) |	Ranges 0.3–0.34 Wh per query, estimated 40 million kWh/day |	No public data |	Data center: up to 130–500 MWh/hr for supercomputers	| No public data |

| Water Usage
per hour	Negligible (mostly metabolic, not cooling)	Estimated 148.28 million liters/day total; ~6.18 million liters/hr globally,
~0.32 ml per query (recent official)	No public data	~162,000 liters/hr for Grok AI, Memphis: ~30,000 gallons/day for cooling	No public data
Temperature	36–37°C (98–99°F), tightly regulated	Operates in data centers cooled to ~18–27°C (64–81°F)	No public data	Data centers typically cooled <28°C (82°F)	No public data

Coverage Limitations
Many LLMs, especially Claude and Llama, do not have public, model-specific data for computation, water, or electricity.
Estimates for LLMs heavily depend on operational scale, hardware choice, data center efficiency, and inference vs. training.
Key Insights
Human brain: Highly efficient, very low environmental impact, runs at constant temperature.
LLMs: Orders of magnitude higher energy and water requirements per operation, dependent on industrial-scale infrastructureture.